{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occurrence = defaultdict(int)\n",
    "character_counts = defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Friends.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Speaker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Speaker'] = df['Speaker'].str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_speakers = df['Speaker'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ross              8869\n",
      "Rachel            8774\n",
      "Chandler          8058\n",
      "Joey              8047\n",
      "Monica            7965\n",
      "Phoebe            7152\n",
      "[Scene            2833\n",
      "Mike               355\n",
      "All                332\n",
      "Rach               325\n",
      "Mnca               259\n",
      "Richard            254\n",
      "Chan               233\n",
      "Janice             208\n",
      "Mr. Geller         204\n",
      "Phoe               195\n",
      "Carol              192\n",
      "Charlie            190\n",
      "Transcribed By     171\n",
      "Mrs. Geller        167\n",
      "Emily              167\n",
      "Tag                146\n",
      "Frank              133\n",
      "Paul               130\n",
      "Gunther            127\n",
      "Written By         126\n",
      "David              120\n",
      "Amy                116\n",
      "Mona               111\n",
      "Woman              105\n",
      "Name: Speaker, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "top_30_characters = df['Speaker'].value_counts().head(30)\n",
    "\n",
    "print(top_30_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries_to_remove = [\"[Scene\", \"All\", \"Transcribed By\", \"Written By\", \"Woman\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['Speaker'].isin(entries_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = {\n",
    "    r'\\bMnca\\b': 'Monica',\n",
    "    r'\\bRach\\b': 'Rachel',\n",
    "    r'\\bPhoe\\b': 'Phoebe',\n",
    "    r'\\bChan\\b': 'Chandler'\n",
    "}\n",
    "\n",
    "df['Speaker'] = df['Speaker'].replace(replacements, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rachel         9099\n",
      "Ross           8869\n",
      "Chandler       8291\n",
      "Monica         8224\n",
      "Joey           8047\n",
      "Phoebe         7347\n",
      "Mike            355\n",
      "Richard         254\n",
      "Janice          208\n",
      "Mr. Geller      204\n",
      "Carol           192\n",
      "Charlie         190\n",
      "Emily           167\n",
      "Mrs. Geller     167\n",
      "Tag             146\n",
      "Frank           133\n",
      "Paul            130\n",
      "Gunther         127\n",
      "David           120\n",
      "Amy             116\n",
      "Mona            111\n",
      "Pete            103\n",
      "Susan           102\n",
      "Joshua           98\n",
      "Gary             96\n",
      "Elizabeth        94\n",
      "Janine           92\n",
      "Kathy            91\n",
      "Jill             83\n",
      "Ben              73\n",
      "Name: Speaker, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "top_30_characters = df['Speaker'].value_counts().head(30)\n",
    "\n",
    "print(top_30_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_30_characters_list = top_30_characters.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Speaker'].isin(top_30_characters_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Monica' 'Joey' 'Chandler' 'Phoebe' 'Ross' 'Rachel' 'Paul' 'Carol'\n",
      " 'Mrs. Geller' 'Mr. Geller' 'Susan' 'Jill' 'David' 'Janice' 'Gunther'\n",
      " 'Richard' 'Ben' 'Frank' 'Mike' 'Pete' 'Kathy' 'Joshua' 'Emily'\n",
      " 'Elizabeth' 'Gary' 'Janine' 'Tag' 'Mona' 'Amy' 'Charlie']\n"
     ]
    }
   ],
   "source": [
    "print(df['Speaker'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occurrence = defaultdict(int)\n",
    "character_counts = defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, episode_group in df.groupby(['Season', 'Episode']):\n",
    "    characters = episode_group['Speaker'].unique()\n",
    "    for character in characters:\n",
    "        character_counts[character] += 1\n",
    "    for char_a, char_b in combinations(characters, 2):\n",
    "        co_occurrence[frozenset([char_a, char_b])] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [{\"Name\": character, \"Weight\": count} for character, count in character_counts.items()]\n",
    "edges = []\n",
    "\n",
    "for characters, count in co_occurrence.items():\n",
    "    char_a, char_b = characters\n",
    "    edges.append({\n",
    "        \"source\": char_a,\n",
    "        \"target\": char_b,\n",
    "        \"weight\": count,\n",
    "        \"sourceIndex\": list(character_counts.keys()).index(char_a),\n",
    "        \"targetIndex\": list(character_counts.keys()).index(char_b)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = {\n",
    "    \"nodes\": nodes,\n",
    "    \"edges\": edges\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('friends-cooccur.json', 'w') as f:\n",
    "    json.dump(final_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Season'] = df['Season'].str.extract(r'(\\d+)').astype(int)\n",
    "df['Episode'] = df['Episode'].str.extract(r'(\\d+)').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3         There's nothing to tell! He's just some guy I...\n",
       "4         C'mon, you're going out with the guy! There's...\n",
       "5          So does he have a hump? A hump and a hairpiece?\n",
       "6                                 Wait, does he eat chalk?\n",
       "8         Just, 'cause, I don't want her to go through ...\n",
       "                               ...                        \n",
       "69966           Yeah! I'm going to Paris. Thank you, Ross!\n",
       "69967                           Yeah, yeah, oh! (They hug)\n",
       "69968                                Oh! Oh, I'm so happy.\n",
       "69969     Then I'm happy too. (They're still hugging - ...\n",
       "69972     Thank you all for coming. We're here today to...\n",
       "Name: Text, Length: 53329, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['WordCount'] = df['Text'].apply(lambda text: len(str(text).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_by_season = df.groupby(['Speaker', 'Season']).WordCount.sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Speaker  Season  WordCount\n",
      "0       Amy       9        720\n",
      "1       Amy      10        721\n",
      "2       Ben       2          5\n",
      "3       Ben       3          8\n",
      "4       Ben       5          9\n",
      "..      ...     ...        ...\n",
      "150   Susan       3         35\n",
      "151   Susan       4         70\n",
      "152   Susan       6         36\n",
      "153     Tag       7       1157\n",
      "154     Tag       8        110\n",
      "\n",
      "[155 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(word_count_by_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_characters = df['Speaker'].unique()\n",
    "unique_seasons = df['Season'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Monica' 'Joey' 'Chandler' 'Phoebe' 'Ross' 'Rachel' 'Paul' 'Carol'\n",
      " 'Mrs. Geller' 'Mr. Geller' 'Susan' 'Jill' 'David' 'Janice' 'Gunther'\n",
      " 'Richard' 'Ben' 'Frank' 'Mike' 'Pete' 'Kathy' 'Joshua' 'Emily'\n",
      " 'Elizabeth' 'Gary' 'Janine' 'Tag' 'Mona' 'Amy' 'Charlie']\n"
     ]
    }
   ],
   "source": [
    "print(unique_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "all_combinations = pd.DataFrame(list(itertools.product(unique_characters, unique_seasons)), columns=['Speaker', 'Season'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_by_season = df.groupby(['Speaker', 'Season']).WordCount.sum().reset_index()\n",
    "\n",
    "complete_data = all_combinations.merge(word_count_by_season, on=['Speaker', 'Season'], how='left').fillna(0)\n",
    "\n",
    "complete_data['WordCount'] = complete_data['WordCount'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Season</th>\n",
       "      <th>WordCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Monica</td>\n",
       "      <td>1</td>\n",
       "      <td>8004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Monica</td>\n",
       "      <td>2</td>\n",
       "      <td>8207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Monica</td>\n",
       "      <td>3</td>\n",
       "      <td>8669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Monica</td>\n",
       "      <td>4</td>\n",
       "      <td>8217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monica</td>\n",
       "      <td>5</td>\n",
       "      <td>9806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>9</td>\n",
       "      <td>1428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>10</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Speaker  Season  WordCount\n",
       "0     Monica       1       8004\n",
       "1     Monica       2       8207\n",
       "2     Monica       3       8669\n",
       "3     Monica       4       8217\n",
       "4     Monica       5       9806\n",
       "..       ...     ...        ...\n",
       "295  Charlie       6          0\n",
       "296  Charlie       7          0\n",
       "297  Charlie       8          0\n",
       "298  Charlie       9       1428\n",
       "299  Charlie      10        504\n",
       "\n",
       "[300 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data.to_csv('word-count-per-season.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/emir/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/emir/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/emir/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /Users/emir/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    filtered_words = [lemmatizer.lemmatize(word) for word in words if word.isalpha() and word not in stop_words]\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ProcessedWords'] = df['Text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_counts_by_character = df.groupby('Speaker')['ProcessedWords'].sum().apply(Counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 10\n",
    "data = []\n",
    "\n",
    "for speaker, counter in word_counts_by_character.items():\n",
    "    top_words_counts = counter.most_common(top_n)\n",
    "    speaker_data = {\"Speaker\": speaker}\n",
    "    speaker_data.update({f\"Word_{i+1}\": word for i, (word, _) in enumerate(top_words_counts)})\n",
    "    speaker_data.update({f\"Count_{i+1}\": count for i, (_, count) in enumerate(top_words_counts)})\n",
    "    data.append(speaker_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_words_data = []\n",
    "\n",
    "for _, row in words_df.iterrows():\n",
    "    speaker = row['Speaker']\n",
    "    words_list = [{'word': row[f'Word_{i+1}'], 'count': row[f'Count_{i+1}']} for i in range(top_n)]\n",
    "    most_common_words_data.append({'character': speaker, 'words': words_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Distinguishing Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Words'] = df['Text'].apply(lambda x: nltk.word_tokenize(x.lower()))\n",
    "df['Words'] = df['Words'].apply(lambda x: [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in x if word.isalpha() and word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Words'] = df['Text'].apply(lambda x: nltk.word_tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Words'] = df['Words'].apply(lambda x: [word for word in x if word.isalpha() and word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ProcessedText'] = df['Words'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = df.groupby('Speaker')['ProcessedText'].apply(' '.join)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=3, max_df=0.85, ngram_range=(1, 2))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpora)\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.todense(), index=corpora.index, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for speaker in corpora.index:\n",
    "    print(f\"{speaker}:\")\n",
    "    speaker_words = tfidf_df.loc[speaker].sort_values(ascending=False).head(10)\n",
    "    print(speaker_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinguishing_words_data = []\n",
    "\n",
    "for speaker in corpora.index:\n",
    "    speaker_words = tfidf_df.loc[speaker].sort_values(ascending=False).head(top_n)\n",
    "    words_list = [{'word': word, 'count': count} for word, count in speaker_words.items()]\n",
    "    distinguishing_words_data.append({'character': speaker, 'words': words_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_words_json = json.dumps(most_common_words_data)\n",
    "distinguishing_words_json = json.dumps(distinguishing_words_data)\n",
    "\n",
    "with open('most_common_words.json', 'w') as f:\n",
    "    f.write(most_common_words_json)\n",
    "\n",
    "with open('distinguishing_words.json', 'w') as f:\n",
    "    f.write(distinguishing_words_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
